{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b69e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yuval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yuval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 20\n",
      "Test set size: 100\n",
      "downloading tokenizer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup, RobertaForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import pipeline\n",
    "from transformers.trainer_utils import number_of_arguments\n",
    "from typing import Optional, Dict, Any, Callable\n",
    "import sentencepiece\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "import setfit\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "import datasets\n",
    "from optuna import Trial\n",
    "\n",
    "d = {}\n",
    "#Creating a custom trainer for evaluating multiple metrics\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyTrainer, self).__init__(**kwargs)\n",
    "\n",
    "    def evaluate(self, dataset: Optional[Dataset] = None, metric_key_prefix: str = \"test\") -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Computes the metrics for a given classifier.\n",
    "\n",
    "        Args:\n",
    "            dataset (`Dataset`, *optional*):\n",
    "                The dataset to compute the metrics on. If not provided, will use the evaluation dataset passed via\n",
    "                the `eval_dataset` argument at `Trainer` initialization.\n",
    "\n",
    "        Returns:\n",
    "            `Dict[str, float]`: The evaluation metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        if dataset is not None:\n",
    "            self._validate_column_mapping(dataset)\n",
    "            if self.column_mapping is not None:\n",
    "                eval_dataset = self._apply_column_mapping(dataset, self.column_mapping)\n",
    "            else:\n",
    "                eval_dataset = dataset\n",
    "        else:\n",
    "            eval_dataset = self.eval_dataset\n",
    "\n",
    "        if eval_dataset is None:\n",
    "            raise ValueError(\"No evaluation dataset provided to `Trainer.evaluate` nor the `Trainer` initialzation.\")\n",
    "\n",
    "        x_test = eval_dataset[\"text\"]\n",
    "        y_test = eval_dataset[\"label\"]\n",
    "\n",
    "        print(\"***** Running evaluation *****\")\n",
    "        y_pred = self.model.predict(x_test, use_labels=False)\n",
    "        if isinstance(y_pred, torch.Tensor):\n",
    "            y_pred = y_pred.cpu()\n",
    "\n",
    "        # Normalize string outputs\n",
    "        if y_test and isinstance(y_test[0], str):\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(list(y_test) + list(y_pred))\n",
    "            y_test = encoder.transform(y_test)\n",
    "            y_pred = encoder.transform(y_pred)\n",
    "\n",
    "        final_results = []\n",
    "        for metric in self.metric:\n",
    "            metric_config = \"multilabel\" if self.model.multi_target_strategy is not None else None\n",
    "            metric_fn = evaluate.load(metric, config_name=metric_config)\n",
    "            if metric != 'accuracy':\n",
    "                metric_kwargs = self.metric_kwargs\n",
    "                results = metric_fn.compute(predictions=y_pred, references=y_test, **metric_kwargs)\n",
    "            else:\n",
    "                metric_kwargs = {}\n",
    "                results = metric_fn.compute(predictions=y_pred, references=y_test, **metric_kwargs)\n",
    "            final_results.append(results)\n",
    "        if not isinstance(results, dict):\n",
    "            results = {\"metric\": results}\n",
    "        self.model.model_card_data.post_training_eval_results(\n",
    "            {f\"{metric_key_prefix}_{key}\": value for key, value in results.items()}\n",
    "        )\n",
    "        return final_results\n",
    "\n",
    "#Creating the dataset class\n",
    "class Create_Data(Dataset):\n",
    "    def __init__(self, data_proc):\n",
    "        self.data = data_proc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.data.iloc[index]['text']\n",
    "        label = self.data.iloc[index]['label']  # Adjust column names if needed\n",
    "        return line, label\n",
    "\n",
    "#parameters for hyperparameters optimization\n",
    "def params_dict(trial: Trial):\n",
    "    return {\n",
    "        \"body_learning_rate\": trial.suggest_float(\"body_learning_rate\", 1e-5, 1e-2, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\",0.0001, 0.01, log=True),\n",
    "        \"num_epochs\": trial.suggest_int(\"num_epochs\", low = 5, high = 20),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [6, 8]),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 3, 42),\n",
    "        \"max_iter\": trial.suggest_int(\"max_iter\", 100, 300),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"newton-cg\", \"lbfgs\", \"liblinear\"]),\n",
    "        'head_learning_rate': trial.suggest_float(\"head_learning_rate\",1e-5, 1e-2, log=True)\n",
    "    }\n",
    "\n",
    "#model init function for the trainer\n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    return SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\", **params).to(device)\n",
    "\n",
    "\n",
    "\n",
    "#preprocessing function- remove stopwords (except for negative word), stemming, removing punctuation marks etc.\n",
    "def preprocess_text(text):\n",
    "    #text preprocessing\n",
    "    # Tokenize the text\n",
    "    if not isinstance(text, str):\n",
    "        tokens = \"\"\n",
    "    else:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    remove_list = ['not','no','off','nor',\"wasn't\",\"haven't\",\"now\",\"isn't\",\"hasn't\",\"don't\",\"doesn't\",\"didn't\",\"couldn't\",\"aren't\",'out','more','than']\n",
    "    for item in remove_list:\n",
    "        stop_words.remove(item)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove punctuation marks\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens =[token.translate(table) for token in tokens if token.translate(table)]\n",
    "\n",
    "    ### Stemming\n",
    "    stemmed = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "\n",
    "            word = stemmer.stem(token)\n",
    "            stemmed.append(word)\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "#assigning the GPU device\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#read the dataset\n",
    "dataset = datasets.load_dataset(\"stanfordnlp/imdb\")\n",
    "df = pd.DataFrame([dataset['train']['text'], dataset['train']['label']]).T.rename(columns = {0:'text', 1:'label'})\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#rename the headers of the dataset\n",
    "df= df.rename(columns = {'remark' : 'text', 'topic':'label'})\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(str)\n",
    "predict_set = df[df['label']==str(np.nan)]\n",
    "df = df[df['label']!=str(np.nan)]\n",
    "\n",
    "#read the test set\n",
    "df_test = pd.read_csv(r\"C:\\Users\\yuval\\OneDrive\\שולחן העבודה\\text classification\\test for setfit.csv\")\n",
    "\n",
    "topics = df[\"label\"].unique()\n",
    "train_set = pd.DataFrame()\n",
    "test_set = pd.DataFrame()\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "#split into train and test\n",
    "for topic in topics:\n",
    "    # select only the rows with the current topic\n",
    "    topic_rows = df[df[\"label\"] == topic]\n",
    "    if len(topic_rows) >= 100:\n",
    "        topic_rows = topic_rows.sample(n=50)\n",
    "    if df.loc[df['label'] == topic].shape[0] < 10:\n",
    "        df = df.loc[df['test']['label'] != topic]\n",
    "        print (fr\"excluded topic number {int(topic)}\")\n",
    "        continue\n",
    "    # split the topic rows into train and test sets\n",
    "    train, test = train_test_split(topic_rows, test_size=0.8)\n",
    "\n",
    "    # add the train and test sets to the overall train and test sets\n",
    "    train_set = pd.concat([train_set, train])\n",
    "    test_set = pd.concat([test_set, test])\n",
    "test_set = df_test\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Test set size:\", len(test_set))\n",
    "remarks = df['text'].apply(lambda x: [str(i) for i in x.split(',')]).tolist()\n",
    "print (\"downloading tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8418e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name SetFit/MiniLM-L12-H384-uncased__sst2__all-train. Creating a new one with MEAN pooling.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pyarrow\\pandas_compat.py:373: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bc31144eee4f9899ea532d98dc0aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num unique pairs = 220\n",
      "  Batch size = 6\n",
      "  Num epochs = 17\n",
      "  Total optimization steps = 629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters are:  {'body_learning_rate': 0.00017157610136629998, 'learning_rate': 0.0006631067948621269, 'num_epochs': 17, 'batch_size': 6, 'seed': 25, 'max_iter': 149, 'solver': 'liblinear', 'head_learning_rate': 0.001640100988107923}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='629' max='629' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [629/629 09:30, Epoch 17/0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "The Evaluation metrics are:  [{'f1': 0.8726655348047538}, {'accuracy': 0.88}, {'precision': 0.8948306595365418}, {'recall': 0.8637110016420362}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read the pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='SetFit/MiniLM-L12-H384-uncased__sst2__all-train')\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "model = SetFitModel.from_pretrained(pretrained_model_name_or_path='SetFit/MiniLM-L12-H384-uncased__sst2__all-train', labels=2)\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "\n",
    "# Prepare training data\n",
    "train_data = Dataset.from_pandas(train_set)\n",
    "val_data = Dataset.from_pandas(test_set)\n",
    "test = test_set.copy()\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience = 3)\n",
    "#assign the trainer class\n",
    "trainer = MyTrainer(\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    model_init=model_init,\n",
    "    metric_kwargs={\"average\": \"macro\"},\n",
    "    # callbacks=[early_stopping],\n",
    "    column_mapping={\"text\": \"text\", \"label\": \"label\"},\n",
    "    metric =['f1', 'accuracy','precision', 'recall']\n",
    ")\n",
    "#a function for the target function to maximize- a combination of the f1 score and accuracy\n",
    "def custom_compute_objective(metrics):\n",
    "    # Option 1: Remove extra argument\n",
    "    f1 = metrics[0]['f1']\n",
    "    accuracy = metrics[1]['accuracy']\n",
    "\n",
    "    return 0.5 * f1 + 0.5 * accuracy\n",
    "\n",
    "trainer.compute_objective = custom_compute_objective\n",
    "\n",
    "#Hyperparameters optimization search, takes long time\n",
    "search = False\n",
    "if search == True:\n",
    "    best_run =trainer.hyperparameter_search( direction=\"maximize\",backend=\"optuna\", hp_space=params_dict,n_trials=30,\n",
    "                                         compute_objective=custom_compute_objective)\n",
    "else:\n",
    "    best_run_path = fr\"C:\\Users\\yuval\\OneDrive\\שולחן העבודה\\text classification\\best_params.json\"\n",
    "    with open(best_run_path, 'r') as json_file:\n",
    "        best_run = json_file.readlines()[0]\n",
    "        best_run = json.loads(best_run)\n",
    "trainer.apply_hyperparameters(best_run, final_model=True)\n",
    "print (\"Best hyperparameters are: \", best_run)\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate(val_data, topics)\n",
    "print(\"The Evaluation metrics are: \", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5031e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = r'C:\\Users\\yuval\\OneDrive\\שולחן העבודה\\text classification\\Setfit_text_classification.pth'\n",
    "\n",
    "# save the model\n",
    "def custom_save_pretrained(model, save_directory):\n",
    "    import os\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    model.labels = ['1', '0']\n",
    "    model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the model to a directory\n",
    "custom_save_pretrained(model, r\"C:\\Users\\yuval\\OneDrive\\שולחן העבודה\\text classification\\SetFit/MiniLM-L12-H384-uncased__sst2__all-train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
